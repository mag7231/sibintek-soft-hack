# train_classifier.py

"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è MobileCLIP –Ω–∞ –∫–∞—Å—Ç–æ–º–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ —Ä–∞–±–æ—Ç–Ω–∏–∫–æ–≤ –∂–µ–ª–µ–∑–Ω–æ–π –¥–æ—Ä–æ–≥–∏.

–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞:
data/workers/
‚îú‚îÄ‚îÄ train/
‚îÇ   ‚îú‚îÄ‚îÄ mechanic/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ img001.jpg
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ img002.jpg
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îú‚îÄ‚îÄ worker/
‚îÇ   ‚îú‚îÄ‚îÄ cleaner/
‚îÇ   ‚îú‚îÄ‚îÄ driver/
‚îÇ   ‚îî‚îÄ‚îÄ unknown/
‚îî‚îÄ‚îÄ val/
    ‚îú‚îÄ‚îÄ mechanic/
    ‚îú‚îÄ‚îÄ worker/
    ‚îú‚îÄ‚îÄ cleaner/
    ‚îú‚îÄ‚îÄ driver/
    ‚îî‚îÄ‚îÄ unknown/
"""

import torch
from torch.utils.data import Dataset, DataLoader
from pathlib import Path
from PIL import Image
import argparse
import yaml

from src.classifier import WorkerClassifier, WorkerClassifierFineTuner


class WorkerDataset(Dataset):
    """
    Dataset –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ —Ä–∞–±–æ—Ç–Ω–∏–∫–æ–≤.
    """
    
    def __init__(self, root_dir: str, transform, tokenizer, classes: list):
        """
        Args:
            root_dir: –ø—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –∫–ª–∞—Å—Å–∞–º–∏
            transform: —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            tokenizer: —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è —Ç–µ–∫—Å—Ç–∞
            classes: —Å–ø–∏—Å–æ–∫ –∫–ª–∞—Å—Å–æ–≤
        """
        self.root_dir = Path(root_dir)
        self.transform = transform
        self.tokenizer = tokenizer
        self.classes = classes
        self.class_to_idx = {cls: idx for idx, cls in enumerate(classes)}
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        self.samples = []
        for class_name in classes:
            class_dir = self.root_dir / class_name
            if not class_dir.exists():
                print(f"‚ö†Ô∏è –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {class_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º...")
                continue
            
            for img_path in class_dir.glob('*.jpg'):
                self.samples.append((img_path, class_name))
            for img_path in class_dir.glob('*.png'):
                self.samples.append((img_path, class_name))
        
        print(f"üìÅ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.samples)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ {root_dir}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∞—Å—Å–∞–º
        class_counts = {}
        for _, class_name in self.samples:
            class_counts[class_name] = class_counts.get(class_name, 0) + 1
        
        for class_name, count in class_counts.items():
            print(f"   - {class_name}: {count} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        img_path, class_name = self.samples[idx]
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        image = Image.open(img_path).convert('RGB')
        image = self.transform(image)
        
        # –¢–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∞
        class_descriptions = {
            'mechanic': 'person wearing white work uniform and safety vest',
            'worker': 'person wearing grey work overalls and hard hat',
            'cleaner': 'person wearing dark blue uniform with orange horizontal stripe on shoulders',
            'driver': 'train driver wearing dark blue professional uniform',
            'unknown': 'person wearing casual clothes'
        }
        
        text = f"a photo of {class_descriptions[class_name]}"
        text_tokens = self.tokenizer([text])[0]
        
        # Label
        label = self.class_to_idx[class_name]
        
        return image, text_tokens, label


def main():
    parser = argparse.ArgumentParser(description='–î–æ–æ–±—É—á–µ–Ω–∏–µ MobileCLIP –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ä–∞–±–æ—Ç–Ω–∏–∫–æ–≤')
    parser.add_argument('--data-root', type=str, default='data/workers',
                       help='–ü—É—Ç—å –∫ –∫–æ—Ä–Ω–µ–≤–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞')
    parser.add_argument('--model-name', type=str, default='hf-hub:Marqo/marqo-fashionCLIP',
                       help='–ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ CLIP')
    parser.add_argument('--batch-size', type=int, default=32,
                       help='–†–∞–∑–º–µ—Ä batch')
    parser.add_argument('--epochs', type=int, default=10,
                       help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö')
    parser.add_argument('--lr', type=float, default=1e-5,
                       help='Learning rate')
    parser.add_argument('--save-path', type=str, default='weights/fashionclip_workers_finetuned.pt',
                       help='–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–µ—Å–æ–≤')
    parser.add_argument('--config', type=str, default='configs/config.yaml',
                       help='–ü—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É')
    
    args = parser.parse_args()
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥–∞
    with open(args.config, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    classifier_config = config.get('classifier', {})
    classes = list(classifier_config.get('classes', {}).keys())
    
    print(f"üéì –ù–∞—á–∏–Ω–∞–µ–º –¥–æ–æ–±—É—á–µ–Ω–∏–µ MobileCLIP")
    print(f"   –ú–æ–¥–µ–ª—å: {args.model_name}")
    print(f"   –î–∞—Ç–∞—Å–µ—Ç: {args.data_root}")
    print(f"   –ö–ª–∞—Å—Å—ã: {classes}")
    print(f"   Batch size: {args.batch_size}")
    print(f"   Epochs: {args.epochs}")
    print(f"   Learning rate: {args.lr}")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
    base_classifier = WorkerClassifier(
        model_name=args.model_name,
        use_fine_tuned=False
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ datasets
    train_dataset = WorkerDataset(
        root_dir=Path(args.data_root) / 'train',
        transform=base_classifier.preprocess,
        tokenizer=base_classifier.tokenizer,
        classes=classes
    )
    
    val_dataset = WorkerDataset(
        root_dir=Path(args.data_root) / 'val',
        transform=base_classifier.preprocess,
        tokenizer=base_classifier.tokenizer,
        classes=classes
    )
    
    if len(train_dataset) == 0:
        print("‚ùå –û—à–∏–±–∫–∞: —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –ø—É—Å—Ç!")
        print(f"   –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–∞—è:")
        print(f"   {args.data_root}/train/<class_name>/*.jpg")
        return
    
    # –°–æ–∑–¥–∞–Ω–∏–µ fine-tuner
    fine_tuner = WorkerClassifierFineTuner(
        base_model=base_classifier,
        learning_rate=args.lr,
        batch_size=args.batch_size,
        num_epochs=args.epochs
    )
    
    # –û–±—É—á–µ–Ω–∏–µ
    fine_tuner.train(
        train_dataset=train_dataset,
        val_dataset=val_dataset if len(val_dataset) > 0 else None,
        save_path=args.save_path
    )
    
    print(f"\n‚úÖ –î–æ–æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"   –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {args.save_path}")
    print(f"\n–î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–æ–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –æ–±–Ω–æ–≤–∏—Ç–µ config.yaml:")
    print(f"   classifier:")
    print(f"     use_fine_tuned: true")
    print(f"     fine_tuned_path: '{args.save_path}'")
    print(f"     model_name: '{args.model_name}'")


if __name__ == '__main__':
    main()